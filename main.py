import os
import re
import json
import tempfile
import subprocess
from typing import List, Optional

import streamlit as st
import requests

# –≤–Ω–µ—à–Ω–∏–µ SDK
from deepgram import Deepgram
import imageio_ffmpeg as ioffmpeg

# ================= UI / page =================
st.set_page_config(page_title="–ò–ò-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ –ò–ò-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å –¥–ª—è –æ–Ω–ª–∞–π–Ω-–≤—Å—Ç—Ä–µ—á")

# ================= helpers =================
def ffmpeg_convert_to_wav(src_path: str, wav_path: str, sr: int = 16000) -> None:
    """–õ—é–±–æ–π –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ ‚Üí WAV 16k mono —á–µ—Ä–µ–∑ ffmpeg (–±–∏–Ω–∞—Ä–Ω–∏–∫ –±–µ—Ä—ë–º –∏–∑ imageio-ffmpeg)."""
    ffmpeg_bin = ioffmpeg.get_ffmpeg_exe()  # –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –µ—Å—Ç—å –Ω–∞ Streamlit Cloud
    cmd = [
        ffmpeg_bin, "-hide_banner", "-y",
        "-i", src_path,
        "-vn",
        "-acodec", "pcm_s16le",
        "-ar", str(sr),
        "-ac", "1",
        wav_path,
    ]
    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if proc.returncode != 0:
        raise RuntimeError("FFmpeg conversion failed: " + proc.stderr.decode(errors="ignore")[:800])

def detect_lang_code(text: str) -> str:
    """–ì—Ä—É–±–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–∏—Ä–∏–ª–ª–∏—Ü—ã."""
    cyr = sum('–∞' <= ch.lower() <= '—è' or ch == '—ë' for ch in text)
    lat = sum('a' <= ch.lower() <= 'z' for ch in text)
    return "ru" if cyr > lat else "en"

def split_sentences(text: str) -> List[str]:
    sents = re.split(r"(?<=[.!?])\s+|[\n\r]+|‚Ä¢\s*| - ", text.strip())
    return [s.strip(" \t-‚Äî‚Ä¢") for s in sents if len(s.strip()) > 2]

def expand_compounds(s: str) -> List[str]:
    parts = re.split(r"\b(–∏|–∞ —Ç–∞–∫–∂–µ|–∑–∞—Ç–µ–º|–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ|–ø–æ—Ç–æ–º|–¥–∞–ª–µ–µ|and then|and)\b", s, flags=re.IGNORECASE)
    out = []
    for p in parts:
        if re.fullmatch(r"(–∏|–∞ —Ç–∞–∫–∂–µ|–∑–∞—Ç–µ–º|–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ|–ø–æ—Ç–æ–º|–¥–∞–ª–µ–µ|and then|and)", p, flags=re.IGNORECASE):
            continue
        frag = p.strip(" ,.;:‚Äî-")
        if frag:
            out.append(frag)
    return out or [s]

# –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –≥–ª–∞–≥–æ–ª–æ–≤-—Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ (RU/EN)
VERB_RE = r"(–ø—Ä–æ–≤–µ—Å—Ç–∏|–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å|–æ—Ç–ø—Ä–∞–≤–∏—Ç—å|—Å–æ–∑–¥–∞—Ç—å|–Ω–∞–ø–∏—Å–∞—Ç—å|–ø—Ä–æ–≤–µ—Ä–∏—Ç—å|—Å–æ–∑–≤–æ–Ω–∏—Ç—å—Å—è|–¥–æ–±–∞–≤–∏—Ç—å|–∏—Å–ø—Ä–∞–≤–∏—Ç—å|–∑–∞–∫—Ä—ã—Ç—å|–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å|—Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å|–æ–±–Ω–æ–≤–∏—Ç—å|–æ–ø–∏—Å–∞—Ç—å|—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å|–ø–æ–¥–∫–ª—é—á–∏—Ç—å|–æ—Ñ–æ—Ä–º–∏—Ç—å|–Ω–∞–∑–Ω–∞—á–∏—Ç—å|–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å|–ø—Ä–µ–∑–µ–Ω—Ç–æ–≤–∞—Ç—å|–æ–∂–∏–¥–∞—Ç—å|—Å–æ–±—Ä–∞—Ç—å|–¥–∞—Ç—å|–≤—ã–ø–æ–ª–Ω–∏—Ç—å|–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å|—É—Ç–≤–µ—Ä–¥–∏—Ç—å|–ø–æ–¥–µ–ª–∏—Ç—å—Å—è|—Å–∫–∏–Ω—É—Ç—å|–∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å|–Ω–∞–ø–æ–º–Ω–∏—Ç—å|–ø–æ–¥–≤–µ—Å—Ç–∏ –∏—Ç–æ–≥–∏|review|plan|schedule|deploy|implement|prepare|send|create|write|check|fix|update|investigate|present|follow up)"

def candidate_actions(text: str) -> List[str]:
    """–í—ã–¥–µ–ª—è–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –∑–∞–¥–∞—á–∏: —Ç–æ–ª—å–∫–æ —Ñ—Ä–∞–∑—ã —Å –≥–ª–∞–≥–æ–ª–∞–º–∏-—Ç—Ä–∏–≥–≥–µ—Ä–∞–º–∏."""
    out = []
    for s in split_sentences(text):
        if not re.search(VERB_RE, s, flags=re.IGNORECASE):
            continue
        for sub in expand_compounds(s):
            sub = re.sub(r"(?i)\b(–Ω—É–∂–Ω–æ|–Ω–∞–¥–æ|–±—É–¥–µ—Ç|–¥–∞–≤–∞–π—Ç–µ|–¥–∞–≤–∞–π|–ø—Ä–µ–¥–ª–∞–≥–∞—é)\s+", "", sub).strip()
            m = re.search(VERB_RE + r".*", sub, flags=re.IGNORECASE)
            frag = sub[m.start():].strip() if m else sub
            frag = re.split(r"[.;!?]", frag)[0].strip(" ,.;:‚Äî-")
            words = frag.split()
            if len(words) > 16:
                frag = " ".join(words[:16])
            if len(frag) >= 3:
                out.append(frag)
    seen, res = set(), []
    for t in out:
        if t not in seen:
            res.append(t); seen.add(t)
    return res

# ============== NLI-—Ñ–∏–ª—å—Ç—Ä (HuggingFace) ==============
@st.cache_resource(show_spinner=False)
def load_nli_models():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–≤–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ (—Å–∫–∞—á–∞—é—Ç—Å—è 1 —Ä–∞–∑):
    - RU: cointegrated/rubert-base-cased-nli-threeway
    - EN: facebook/bart-large-mnli
    –ï—Å–ª–∏ –≤ –ø–∞–ø–∫–µ models/nli-ru –∏–ª–∏ models/nli-en –ª–µ–∂–∞—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –∫–æ–ø–∏–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö.
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    models = {}

    ru_dir = "models/nli-ru"
    ru_name = "cointegrated/rubert-base-cased-nli-threeway"
    try:
        tok_ru = AutoTokenizer.from_pretrained(ru_dir if os.path.isdir(ru_dir) else ru_name,
                                               local_files_only=os.path.isdir(ru_dir))
        mdl_ru = AutoModelForSequenceClassification.from_pretrained(ru_dir if os.path.isdir(ru_dir) else ru_name,
                                                                    local_files_only=os.path.isdir(ru_dir))
        models["ru"] = (tok_ru, mdl_ru)
    except Exception:
        models["ru"] = None

    en_dir = "models/nli-en"
    en_name = "facebook/bart-large-mnli"
    try:
        tok_en = AutoTokenizer.from_pretrained(en_dir if os.path.isdir(en_dir) else en_name,
                                               local_files_only=os.path.isdir(en_dir))
        mdl_en = AutoModelForSequenceClassification.from_pretrained(en_dir if os.path.isdir(en_dir) else en_name,
                                                                    local_files_only=os.path.isdir(en_dir))
        models["en"] = (tok_en, mdl_en)
    except Exception:
        models["en"] = None

    return models

import torch

def nli_is_task(premise: str, lang: str, models, threshold: float = 0.60) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—É '—ç—Ç–æ –ø–æ—Ä—É—á–µ–Ω–∏–µ/task' —á–µ—Ä–µ–∑ NLI. True, –µ—Å–ª–∏ entailment >= threshold."""
    pair = models.get("ru" if lang == "ru" else "en")
    if not pair:
        return True  # –Ω–µ—Ç –º–æ–¥–µ–ª–∏ ‚Üí –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º

    tok, mdl = pair
    hypothesis = "–≠—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å." if lang == "ru" else \
                 "This is an actionable task to be done."
    inputs = tok([premise], [hypothesis], return_tensors="pt", truncation=True, padding=True, max_length=256)
    with torch.no_grad():
        logits = mdl(**inputs).logits[0]
    probs = torch.softmax(logits, dim=-1)
    entail_p = float(probs[-1])  # –ø–æ—Å–ª–µ–¥–Ω–∏–π ‚Äî entailment
    return entail_p >= threshold

def extract_tasks_with_nli(text: str) -> List[str]:
    lang = detect_lang_code(text)
    cand = candidate_actions(text)
    if not cand:
        return []
    models = load_nli_models()

    out = []
    bar = st.progress(0, text="–§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∑–∞–¥–∞—á (NLI)‚Ä¶")
    total = len(cand)
    for i, c in enumerate(cand, 1):
        if nli_is_task(c, lang, models):
            c2 = re.sub(r"(?i)^(–ø—Ä–æ—à—É|–Ω—É–∂–Ω–æ|–Ω–∞–¥–æ|–±—É–¥–µ—Ç|–¥–∞–≤–∞–π—Ç–µ|–¥–∞–≤–∞–π)\s+", "", c).strip(" -‚Äî‚Ä¢")
            c2 = re.sub(r"[\.!\s]+$", "", c2)
            if len(c2) > 140:
                c2 = c2[:140]
            if c2:
                out.append(c2)
        bar.progress(i/total)
    # –¥–µ–¥—É–ø
    seen, res = set(), []
    for t in out:
        if t and t not in seen:
            res.append(t); seen.add(t)
    return res

def _adf_paragraph(text: str):
    return {"type": "paragraph", "content": [{"type": "text", "text": text}]}

def create_jira_task(base_url: str, email: str, token: str, project_key: str,
                     summary: str, description: Optional[str] = None) -> str:
    """–°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É –≤ Jira Cloud (API v3) c ADF-–æ–ø–∏—Å–∞–Ω–∏–µ–º."""
    url = f"{base_url.rstrip('/')}/rest/api/3/issue"
    auth = (email, token)
    headers = {"Accept": "application/json", "Content-Type": "application/json"}

    adf_desc = {
        "type": "doc",
        "version": 1,
        "content": [
            _adf_paragraph(description or ""),
            _adf_paragraph("‚Äî —Å–æ–∑–¥–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: AI Secretary (Deepgram ‚Üí Jira)"),
        ],
    }

    payload = {
        "fields": {
            "project": {"key": project_key},
            "summary": (summary or "Task")[:254],
            "issuetype": {"name": "Task"},
            "description": adf_desc,
        }
    }
    r = requests.post(url, auth=auth, headers=headers, json=payload, timeout=30)
    if r.status_code not in (200, 201):
        raise RuntimeError(f"Jira error {r.status_code}: {r.text[:600]}")
    return r.json().get("key", "?")

# ================= UI: –∑–∞–≥—Ä—É–∑–∫–∞ =================
up = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ (mp3, wav, ogg, m4a, mp4, mkv)",
    type=["mp3", "wav", "ogg", "m4a", "mp4", "mkv"]
)

# ================= –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ =================
if up:
    st.audio(up.getvalue())
    if st.button("üéôÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∏ –≤—ã–¥–µ–ª–∏—Ç—å –∑–∞–¥–∞—á–∏"):
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏–º
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(up.name)[1]) as tmp:
            tmp.write(up.getvalue())
            src_path = tmp.name
        wav_path = src_path + ".wav"

        with st.spinner("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV (16kHz mono)‚Ä¶"):
            ffmpeg_convert_to_wav(src_path, wav_path)

        # Deepgram –∫–ª—é—á –∏–∑ Streamlit Secrets (–∏–ª–∏ –∏–∑ env –¥–ª—è –ª–æ–∫–∞–ª–∫–∏)
        DG_KEY = st.secrets.get("DEEPGRAM_API_KEY") or os.getenv("DEEPGRAM_API_KEY")
        if not DG_KEY:
            st.error("‚ùå –ù–µ—Ç –∫–ª—é—á–∞ Deepgram. –î–æ–±–∞–≤—å—Ç–µ DEEPGRAM_API_KEY –≤ Secrets (Streamlit Cloud) –∏–ª–∏ env.")
            st.stop()

        dg = Deepgram(DG_KEY)

        with st.spinner("–†–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤ Deepgram‚Ä¶"):
            with open(wav_path, "rb") as f:
                source = {"buffer": f, "mimetype": "audio/wav"}
                res = dg.transcription.sync_prerecorded(
                    source,
                    {"punctuate": True, "smart_format": True, "language": "ru"}  # –º–æ–∂–Ω–æ "auto" –µ—Å–ª–∏ –±—É–¥–µ—Ç –º–Ω–æ–≥–æ EN
                )

        transcript = res["results"]["channels"][0]["alternatives"][0]["transcript"].strip()
        st.session_state["transcript"] = transcript

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ NLI
        st.info("üß† –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á–∏ (NLI-—Ñ–∏–ª—å—Ç—Ä, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)‚Ä¶")
        st.session_state["tasks"] = extract_tasks_with_nli(transcript)

# ================= –í—ã–≤–æ–¥ =================
if "transcript" in st.session_state:
    st.subheader("üìÑ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
    st.text_area("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç", st.session_state["transcript"], height=220)

if "tasks" in st.session_state:
    st.subheader("üìå –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏")
    tasks = st.session_state["tasks"]
    if tasks:
        for i, t in enumerate(tasks, 1):
            st.markdown(f"- **{i}.** {t}")
    else:
        st.info("–ù–µ –Ω–∞—à—ë–ª —è–≤–Ω—ã—Ö –ø–æ—Ä—É—á–µ–Ω–∏–π. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Ñ—Ä–∞–∑—ã —Å –≥–ª–∞–≥–æ–ª–∞–º–∏ –¥–µ–π—Å—Ç–≤–∏—è.")

    # ===== Jira =====
    st.divider()
    st.subheader("üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–¥–∞—á –≤ Jira")
    with st.form("jira_form"):
        jira_base   = st.text_input("Jira Base URL", placeholder="https://your-org.atlassian.net")
        jira_email  = st.text_input("Jira Email", placeholder="you@example.com")
        jira_token  = st.text_input("Jira API Token", type="password")
        jira_proj   = st.text_input("Project Key", placeholder="TEST")
        send = st.form_submit_button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ Jira ‚úÖ")

    if send:
        if not (jira_base and jira_email and jira_token and jira_proj):
            st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è Jira.")
        else:
            results = []
            bar = st.progress(0, text="–°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –≤ Jira‚Ä¶")
            total = max(1, len(tasks))
            for i, task in enumerate(tasks, 1):
                try:
                    key = create_jira_task(jira_base, jira_email, jira_token, jira_proj,
                                           summary=task, description=task)
                    results.append(("ok", key, task))
                except Exception as e:
                    results.append(("err", str(e), task))
                bar.progress(i/total)
            st.subheader("üßæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á")
            ok = [r for r in results if r[0] == "ok"]
            err = [r for r in results if r[0] == "err"]
            if ok:
                st.success("–°–æ–∑–¥–∞–Ω—ã: " + ", ".join([k for _, k, _ in ok]))
            for _, msg, t in err:
                st.error(f"–ù–µ —Å–æ–∑–¥–∞–Ω–∞: ‚Äû{t}‚Äú ‚Üí {msg}")
