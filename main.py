import os
import re
import json
import tempfile
import subprocess
from typing import List, Optional

import streamlit as st
import requests
from dotenv import load_dotenv
load_dotenv()

# ============== –≤–Ω–µ—à–Ω–∏–µ SDK ==============
from deepgram import Deepgram

# ============== UI / page ==============
st.set_page_config(page_title="–ò–ò-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ –ò–ò-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å –¥–ª—è –æ–Ω–ª–∞–π–Ω-–≤—Å—Ç—Ä–µ—á")

# ============== helpers ==============
def ffmpeg_convert_to_wav(src_path: str, wav_path: str, sr: int = 16000) -> None:
    """–õ—é–±–æ–π –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ ‚Üí WAV 16k mono —á–µ—Ä–µ–∑ ffmpeg."""
    cmd = [
        "ffmpeg", "-hide_banner", "-y",
        "-i", src_path,
        "-vn",
        "-acodec", "pcm_s16le",
        "-ar", str(sr),
        "-ac", "1",
        wav_path,
    ]
    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if proc.returncode != 0:
        raise RuntimeError("FFmpeg conversion failed: " + proc.stderr.decode(errors="ignore")[:800])

def detect_lang_code(text: str) -> str:
    """–ì—Ä—É–±–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–∏—Ä–∏–ª–ª–∏—Ü—ã."""
    cyr = sum('–∞' <= ch.lower() <= '—è' or ch == '—ë' for ch in text)
    lat = sum('a' <= ch.lower() <= 'z' for ch in text)
    if cyr > lat:
        return "ru"
    return "en"

def split_sentences(text: str) -> List[str]:
    # —Ç–æ—á–µ—á–∫–∏/–≤–æ—Å–∫–ª./–≤–æ–ø—Ä–æ—Å/–ø–µ—Ä–µ–Ω–æ—Å—ã/–±—É–ª–ª–µ—Ç—ã/–¥–µ—Ñ–∏—Å—ã
    sents = re.split(r"(?<=[.!?])\s+|[\n\r]+|‚Ä¢\s*| - ", text.strip())
    return [s.strip(" \t-‚Äî‚Ä¢") for s in sents if len(s.strip()) > 2]

def expand_compounds(s: str) -> List[str]:
    parts = re.split(r"\b(–∏|–∞ —Ç–∞–∫–∂–µ|–∑–∞—Ç–µ–º|–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ|–ø–æ—Ç–æ–º|–¥–∞–ª–µ–µ|and then|and)\b", s, flags=re.IGNORECASE)
    # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –∫—É—Å–∫–∏
    out = []
    buf = ""
    for p in parts:
        if re.fullmatch(r"(–∏|–∞ —Ç–∞–∫–∂–µ|–∑–∞—Ç–µ–º|–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ|–ø–æ—Ç–æ–º|–¥–∞–ª–µ–µ|and then|and)", p, flags=re.IGNORECASE):
            buf += " "
        else:
            frag = p.strip(" ,.;:‚Äî-")
            if frag:
                out.append(frag)
    return out or [s]

# –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –≥–ª–∞–≥–æ–ª–æ–≤-—Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ (RU/EN)
VERB_RE = r"(–ø—Ä–æ–≤–µ—Å—Ç–∏|–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å|–æ—Ç–ø—Ä–∞–≤–∏—Ç—å|—Å–æ–∑–¥–∞—Ç—å|–Ω–∞–ø–∏—Å–∞—Ç—å|–ø—Ä–æ–≤–µ—Ä–∏—Ç—å|—Å–æ–∑–≤–æ–Ω–∏—Ç—å—Å—è|–¥–æ–±–∞–≤–∏—Ç—å|–∏—Å–ø—Ä–∞–≤–∏—Ç—å|–∑–∞–∫—Ä—ã—Ç—å|–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å|—Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å|–æ–±–Ω–æ–≤–∏—Ç—å|–æ–ø–∏—Å–∞—Ç—å|—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å|–ø–æ–¥–∫–ª—é—á–∏—Ç—å|–æ—Ñ–æ—Ä–º–∏—Ç—å|–Ω–∞–∑–Ω–∞—á–∏—Ç—å|–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å|–ø—Ä–µ–∑–µ–Ω—Ç–æ–≤–∞—Ç—å|–æ–∂–∏–¥–∞—Ç—å|—Å–æ–±—Ä–∞—Ç—å|–¥–∞—Ç—å|–≤—ã–ø–æ–ª–Ω–∏—Ç—å|–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å|—É—Ç–≤–µ—Ä–¥–∏—Ç—å|–ø–æ–¥–µ–ª–∏—Ç—å—Å—è|—Å–∫–∏–Ω—É—Ç—å|–∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å|–Ω–∞–ø–æ–º–Ω–∏—Ç—å|–ø–æ–¥–≤–µ—Å—Ç–∏ –∏—Ç–æ–≥–∏|—Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å|review|plan|schedule|deploy|implement|prepare|send|create|write|check|fix|update|investigate|present|follow up)"

def candidate_actions(text: str) -> List[str]:
    """–í—ã–¥–µ–ª—è–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –∑–∞–¥–∞—á–∏: —Ç–æ–ª—å–∫–æ —Ñ—Ä–∞–∑—ã —Å –≥–ª–∞–≥–æ–ª–∞–º–∏-—Ç—Ä–∏–≥–≥–µ—Ä–∞–º–∏."""
    out = []
    for s in split_sentences(text):
        if not re.search(VERB_RE, s, flags=re.IGNORECASE):
            continue
        # —Ä–µ–∂–µ–º –Ω–∞ –ø–æ–¥-—Ñ—Ä–∞–∑—ã
        for sub in expand_compounds(s):
            # –≤—ã—Ä–µ–∑–∞–µ–º –≤–≤–æ–¥–Ω—ã–µ —Ç–∏–ø–∞ '–Ω—É–∂–Ω–æ/–Ω–∞–¥–æ/–±—É–¥–µ—Ç'
            sub = re.sub(r"(?i)\b(–Ω—É–∂–Ω–æ|–Ω–∞–¥–æ|–±—É–¥–µ—Ç|–¥–∞–≤–∞–π—Ç–µ|–¥–∞–≤–∞–π|–ø—Ä–µ–¥–ª–∞–≥–∞—é)\s+", "", sub).strip()
            # –±–µ—Ä—ë–º —á–∞—Å—Ç—å –Ω–∞—á–∏–Ω–∞—è —Å –≥–ª–∞–≥–æ–ª–∞-—Ç—Ä–∏–≥–≥–µ—Ä–∞
            m = re.search(VERB_RE + r".*", sub, flags=re.IGNORECASE)
            frag = sub[m.start():].strip() if m else sub
            # –æ–±—Ä–µ–∑–∞–µ–º –ø–æ –ø–µ—Ä–≤–æ–º—É —Å–∏–ª—å–Ω–æ–º—É —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é
            frag = re.split(r"[.;!?]", frag)[0].strip(" ,.;:‚Äî-")
            # –∫–æ—Ä–æ—Ç–∫–æ
            words = frag.split()
            if len(words) > 16:
                frag = " ".join(words[:16])
            if len(frag) >= 3:
                out.append(frag)
    # –¥–µ–¥—É–ø
    seen, res = set(), []
    for t in out:
        if t not in seen:
            res.append(t); seen.add(t)
    return res

# ============== NLI-—Ñ–∏–ª—å—Ç—Ä (HuggingFace) ==============
@st.cache_resource(show_spinner=False)
def load_nli_models():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–≤–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ (—Å–∫–∞—á–∞–µ—Ç 1 —Ä–∞–∑):
    - RU: cointegrated/rubert-base-cased-nli-threeway
    - EN: facebook/bart-large-mnli
    –ï—Å–ª–∏ –≤ –ø–∞–ø–∫–µ models/nli-ru –∏–ª–∏ models/nli-en –ª–µ–∂–∞—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –∫–æ–ø–∏–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö.
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    models = {}

    # RU
    ru_dir = "models/nli-ru"
    ru_name = "cointegrated/rubert-base-cased-nli-threeway"
    try:
        tok_ru = AutoTokenizer.from_pretrained(ru_dir if os.path.isdir(ru_dir) else ru_name, local_files_only=os.path.isdir(ru_dir))
        mdl_ru = AutoModelForSequenceClassification.from_pretrained(ru_dir if os.path.isdir(ru_dir) else ru_name, local_files_only=os.path.isdir(ru_dir))
        models["ru"] = (tok_ru, mdl_ru)
    except Exception as e:
        models["ru"] = None

    # EN
    en_dir = "models/nli-en"
    en_name = "facebook/bart-large-mnli"
    try:
        tok_en = AutoTokenizer.from_pretrained(en_dir if os.path.isdir(en_dir) else en_name, local_files_only=os.path.isdir(en_dir))
        mdl_en = AutoModelForSequenceClassification.from_pretrained(en_dir if os.path.isdir(en_dir) else en_name, local_files_only=os.path.isdir(en_dir))
        models["en"] = (tok_en, mdl_en)
    except Exception as e:
        models["en"] = None

    return models

import torch

def nli_is_task(premise: str, lang: str, models, threshold: float = 0.60) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—É '—ç—Ç–æ –ø–æ—Ä—É—á–µ–Ω–∏–µ/task' —á–µ—Ä–µ–∑ NLI.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å ENTAILMENT >= threshold.
    """
    pair = models.get("ru" if lang == "ru" else "en")
    if not pair:
        return True  # –µ—Å–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ—Ç ‚Äî –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º, –ø–æ–π–¥—ë–º –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º

    tok, mdl = pair
    if lang == "ru":
        hypothesis = "–≠—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ (–∑–∞–¥–∞—á–∞), –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å."
    else:
        hypothesis = "This is an actionable task to be done."

    inputs = tok([premise], [hypothesis], return_tensors="pt", truncation=True, padding=True, max_length=256)
    with torch.no_grad():
        logits = mdl(**inputs).logits[0]
    # —É ru-nli —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞: contradiction, neutral, entailment (–æ–±—ã—á–Ω–æ –ø–æ—Ä—è–¥–æ–∫ 0/1/2)
    # —É bart-large-mnli: contradiction, neutral, entailment —Ç–æ–∂–µ –≤ —Ç–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ
    probs = torch.softmax(logits, dim=-1)
    entail_p = float(probs[-1])  # –ø–æ—Å–ª–µ–¥–Ω–∏–π ‚Äî entailment
    return entail_p >= threshold

def extract_tasks_with_nli(text: str) -> List[str]:
    lang = detect_lang_code(text)
    cand = candidate_actions(text)
    if not cand:
        return []
    models = load_nli_models()

    out = []
    bar = st.progress(0, text="–§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∑–∞–¥–∞—á (NLI)‚Ä¶")
    total = len(cand)
    for i, c in enumerate(cand, 1):
        ok = nli_is_task(c, lang, models)
        if ok:
            # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –∏–º–ø–µ—Ä–∞—Ç–∏–≤
            c2 = re.sub(r"(?i)^(–ø—Ä–æ—à—É|–Ω—É–∂–Ω–æ|–Ω–∞–¥–æ|–±—É–¥–µ—Ç|–¥–∞–≤–∞–π—Ç–µ|–¥–∞–≤–∞–π)\s+", "", c).strip(" -‚Äî‚Ä¢")
            c2 = re.sub(r"[\.!\s]+$", "", c2)
            out.append(c2)
        bar.progress(i/total)
    # –¥–µ–¥—É–ø, —É–∫–æ—Ä–æ—Ç–∏—Ç—å
    seen, res = set(), []
    for t in out:
        if len(t) > 140:
            t = t[:140]
        if t and t not in seen:
            res.append(t); seen.add(t)
    return res

def _adf_paragraph(text: str):
    return {"type": "paragraph", "content": [{"type": "text", "text": text}]}

def create_jira_task(base_url: str, email: str, token: str, project_key: str, summary: str, description: Optional[str] = None) -> str:
    """–°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É –≤ Jira Cloud (API v3) c ADF-–æ–ø–∏—Å–∞–Ω–∏–µ–º."""
    url = f"{base_url.rstrip('/')}/rest/api/3/issue"
    auth = (email, token)
    headers = {"Accept": "application/json", "Content-Type": "application/json"}

    adf_desc = {
        "type": "doc",
        "version": 1,
        "content": [
            _adf_paragraph(description or ""),
            _adf_paragraph("‚Äî —Å–æ–∑–¥–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: AI Secretary (Deepgram ‚Üí Jira)"),
        ],
    }

    payload = {
        "fields": {
            "project": {"key": project_key},
            "summary": (summary or "Task")[:254],
            "issuetype": {"name": "Task"},
            "description": adf_desc,
        }
    }
    r = requests.post(url, auth=auth, headers=headers, json=payload, timeout=30)
    if r.status_code not in (200, 201):
        raise RuntimeError(f"Jira error {r.status_code}: {r.text[:600]}")
    return r.json().get("key", "?")

# ============== UI: –∑–∞–≥—Ä—É–∑–∫–∞ ==============
up = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ (mp3, wav, ogg, m4a, mp4, mkv)",
    type=["mp3", "wav", "ogg", "m4a", "mp4", "mkv"]
)

# ============== –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ==============
if up:
    st.audio(up.getvalue())
    if st.button("üéôÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∏ –≤—ã–¥–µ–ª–∏—Ç—å –∑–∞–¥–∞—á–∏"):
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏–º
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(up.name)[1]) as tmp:
            tmp.write(up.getvalue())
            src_path = tmp.name
        wav_path = src_path + ".wav"

        with st.spinner("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV (16kHz mono)‚Ä¶"):
            ffmpeg_convert_to_wav(src_path, wav_path)

        # Deepgram v2
        DG_KEY = os.getenv("DEEPGRAM_API_KEY")
        if not DG_KEY:
            st.error("‚ùå –ù–µ—Ç –∫–ª—é—á–∞ Deepgram. –î–æ–±–∞–≤—å—Ç–µ DEEPGRAM_API_KEY –≤ .env/Secrets.")
            st.stop()
        dg = Deepgram(DG_KEY)

        with st.spinner("–†–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤ Deepgram‚Ä¶"):
            with open(wav_path, "rb") as f:
                source = {"buffer": f, "mimetype": "audio/wav"}
                res = dg.transcription.sync_prerecorded(source, {"punctuate": True, "smart_format": True, "language": "ru"})

        transcript = res["results"]["channels"][0]["alternatives"][0]["transcript"].strip()
        st.session_state["transcript"] = transcript

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ NLI
        st.info("üß† –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á–∏ (NLI-—Ñ–∏–ª—å—Ç—Ä, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)‚Ä¶")
        st.session_state["tasks"] = extract_tasks_with_nli(transcript)

# ============== –í—ã–≤–æ–¥ ==============
if "transcript" in st.session_state:
    st.subheader("üìÑ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
    st.text_area("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç", st.session_state["transcript"], height=220)

if "tasks" in st.session_state:
    st.subheader("üìå –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏")
    tasks = st.session_state["tasks"]
    if tasks:
        for i, t in enumerate(tasks, 1):
            st.markdown(f"- **{i}.** {t}")
    else:
        st.info("–ù–µ –Ω–∞—à—ë–ª —è–≤–Ω—ã—Ö –ø–æ—Ä—É—á–µ–Ω–∏–π. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Ñ—Ä–∞–∑—ã —Å –≥–ª–∞–≥–æ–ª–∞–º–∏ –¥–µ–π—Å—Ç–≤–∏—è.")

    # ============== Jira ==============
    st.divider()
    st.subheader("üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–¥–∞—á –≤ Jira")
    with st.form("jira_form"):
        jira_base   = st.text_input("Jira Base URL", placeholder="https://your-org.atlassian.net")
        jira_email  = st.text_input("Jira Email", placeholder="you@example.com")
        jira_token  = st.text_input("Jira API Token", type="password")
        jira_proj   = st.text_input("Project Key", placeholder="TEST")
        send = st.form_submit_button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ Jira ‚úÖ")

    if send:
        if not (jira_base and jira_email and jira_token and jira_proj):
            st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è Jira.")
        else:
            results = []
            bar = st.progress(0, text="–°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –≤ Jira‚Ä¶")
            total = max(1, len(tasks))
            for i, task in enumerate(tasks, 1):
                try:
                    key = create_jira_task(jira_base, jira_email, jira_token, jira_proj, summary=task, description=task)
                    results.append(("ok", key, task))
                except Exception as e:
                    results.append(("err", str(e), task))
                bar.progress(i/total)
            st.subheader("üßæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á")
            ok = [r for r in results if r[0] == "ok"]
            err = [r for r in results if r[0] == "err"]
            if ok:
                st.success("–°–æ–∑–¥–∞–Ω—ã: " + ", ".join([k for _, k, _ in ok]))
            for _, msg, t in err:
                st.error(f"–ù–µ —Å–æ–∑–¥–∞–Ω–∞: ‚Äû{t}‚Äú ‚Üí {msg}")
