import os
import re
import io
import json
import tempfile
from typing import List, Optional

import streamlit as st
import requests
from deepgram import Deepgram

# ================= UI / page =================
st.set_page_config(page_title="–ò–ò-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å", page_icon="ü§ñ", layout="wide")
st.title("ü§ñ –ò–ò-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å –¥–ª—è –æ–Ω–ª–∞–π–Ω-–≤—Å—Ç—Ä–µ—á")

# ================= helpers =================
def detect_lang_code(text: str) -> str:
    cyr = sum('–∞' <= ch.lower() <= '—è' or ch == '—ë' for ch in text)
    lat = sum('a' <= ch.lower() <= 'z' for ch in text)
    return "ru" if cyr > lat else "en"

def split_sentences(text: str) -> List[str]:
    sents = re.split(r"(?<=[.!?])\s+|[\n\r]+|‚Ä¢\s*| - ", text.strip())
    return [s.strip(" \t-‚Äî‚Ä¢") for s in sents if len(s.strip()) > 2]

def expand_compounds(s: str) -> List[str]:
    parts = re.split(r"\b(–∏|–∞ —Ç–∞–∫–∂–µ|–∑–∞—Ç–µ–º|–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ|–ø–æ—Ç–æ–º|–¥–∞–ª–µ–µ|and then|and)\b", s, flags=re.IGNORECASE)
    out = []
    for p in parts:
        if re.fullmatch(r"(–∏|–∞ —Ç–∞–∫–∂–µ|–∑–∞—Ç–µ–º|–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ|–ø–æ—Ç–æ–º|–¥–∞–ª–µ–µ|and then|and)", p, flags=re.IGNORECASE):
            continue
        frag = p.strip(" ,.;:‚Äî-")
        if frag:
            out.append(frag)
    return out or [s]

VERB_RE = r"(–ø—Ä–æ–≤–µ—Å—Ç–∏|–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å|–æ—Ç–ø—Ä–∞–≤–∏—Ç—å|—Å–æ–∑–¥–∞—Ç—å|–Ω–∞–ø–∏—Å–∞—Ç—å|–ø—Ä–æ–≤–µ—Ä–∏—Ç—å|—Å–æ–∑–≤–æ–Ω–∏—Ç—å—Å—è|–¥–æ–±–∞–≤–∏—Ç—å|–∏—Å–ø—Ä–∞–≤–∏—Ç—å|–∑–∞–∫—Ä—ã—Ç—å|–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å|—Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å|–æ–±–Ω–æ–≤–∏—Ç—å|–æ–ø–∏—Å–∞—Ç—å|—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å|–ø–æ–¥–∫–ª—é—á–∏—Ç—å|–æ—Ñ–æ—Ä–º–∏—Ç—å|–Ω–∞–∑–Ω–∞—á–∏—Ç—å|–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å|–ø—Ä–µ–∑–µ–Ω—Ç–æ–≤–∞—Ç—å|–æ–∂–∏–¥–∞—Ç—å|—Å–æ–±—Ä–∞—Ç—å|–¥–∞—Ç—å|–≤—ã–ø–æ–ª–Ω–∏—Ç—å|–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å|—É—Ç–≤–µ—Ä–¥–∏—Ç—å|–ø–æ–¥–µ–ª–∏—Ç—å—Å—è|—Å–∫–∏–Ω—É—Ç—å|–∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å|–Ω–∞–ø–æ–º–Ω–∏—Ç—å|–ø–æ–¥–≤–µ—Å—Ç–∏ –∏—Ç–æ–≥–∏|review|plan|schedule|deploy|implement|prepare|send|create|write|check|fix|update|investigate|present|follow up)"

def candidate_actions(text: str) -> List[str]:
    out = []
    for s in split_sentences(text):
        if not re.search(VERB_RE, s, flags=re.IGNORECASE):
            continue
        for sub in expand_compounds(s):
            sub = re.sub(r"(?i)\b(–Ω—É–∂–Ω–æ|–Ω–∞–¥–æ|–±—É–¥–µ—Ç|–¥–∞–≤–∞–π—Ç–µ|–¥–∞–≤–∞–π|–ø—Ä–µ–¥–ª–∞–≥–∞—é)\s+", "", sub).strip()
            m = re.search(VERB_RE + r".*", sub, flags=re.IGNORECASE)
            frag = sub[m.start():].strip() if m else sub
            frag = re.split(r"[.;!?]", frag)[0].strip(" ,.;:‚Äî-")
            words = frag.split()
            if len(words) > 16:
                frag = " ".join(words[:16])
            if len(frag) >= 3:
                out.append(frag)
    seen, res = set(), []
    for t in out:
        if t not in seen:
            res.append(t); seen.add(t)
    return res

@st.cache_resource(show_spinner=False)
def load_nli_models():
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    models = {}
    ru_dir = "models/nli-ru"
    ru_name = "cointegrated/rubert-base-cased-nli-threeway"
    try:
        tok_ru = AutoTokenizer.from_pretrained(ru_dir if os.path.isdir(ru_dir) else ru_name,
                                               local_files_only=os.path.isdir(ru_dir))
        mdl_ru = AutoModelForSequenceClassification.from_pretrained(ru_dir if os.path.isdir(ru_dir) else ru_name,
                                                                    local_files_only=os.path.isdir(ru_dir))
        models["ru"] = (tok_ru, mdl_ru)
    except Exception:
        models["ru"] = None

    en_dir = "models/nli-en"
    en_name = "facebook/bart-large-mnli"
    try:
        tok_en = AutoTokenizer.from_pretrained(en_dir if os.path.isdir(en_dir) else en_name,
                                               local_files_only=os.path.isdir(en_dir))
        mdl_en = AutoModelForSequenceClassification.from_pretrained(en_dir if os.path.isdir(en_dir) else en_name,
                                                                    local_files_only=os.path.isdir(en_dir))
        models["en"] = (tok_en, mdl_en)
    except Exception:
        models["en"] = None

    return models

import torch

def nli_is_task(premise: str, lang: str, models, threshold: float = 0.60) -> bool:
    pair = models.get("ru" if lang == "ru" else "en")
    if not pair:
        return True
    tok, mdl = pair
    hypothesis = "–≠—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –ø–æ—Ä—É—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å." if lang == "ru" else \
                 "This is an actionable task to be done."
    inputs = tok([premise], [hypothesis], return_tensors="pt", truncation=True, padding=True, max_length=256)
    with torch.no_grad():
        logits = mdl(**inputs).logits[0]
    probs = torch.softmax(logits, dim=-1)
    entail_p = float(probs[-1])
    return entail_p >= threshold

def extract_tasks_with_nli(text: str) -> List[str]:
    lang = detect_lang_code(text)
    cand = candidate_actions(text)
    if not cand:
        return []
    models = load_nli_models()

    out = []
    bar = st.progress(0, text="–§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∑–∞–¥–∞—á (NLI)‚Ä¶")
    total = len(cand)
    for i, c in enumerate(cand, 1):
        if nli_is_task(c, lang, models):
            c2 = re.sub(r"(?i)^(–ø—Ä–æ—à—É|–Ω—É–∂–Ω–æ|–Ω–∞–¥–æ|–±—É–¥–µ—Ç|–¥–∞–≤–∞–π—Ç–µ|–¥–∞–≤–∞–π)\s+", "", c).strip(" -‚Äî‚Ä¢")
            c2 = re.sub(r"[\.!\s]+$", "", c2)
            if len(c2) > 140:
                c2 = c2[:140]
            if c2:
                out.append(c2)
        bar.progress(i/total)
    seen, res = set(), []
    for t in out:
        if t and t not in seen:
            res.append(t); seen.add(t)
    return res

def _adf_paragraph(text: str):
    return {"type": "paragraph", "content": [{"type": "text", "text": text}]}

def create_jira_task(base_url: str, email: str, token: str, project_key: str,
                     summary: str, description: Optional[str] = None) -> str:
    url = f"{base_url.rstrip('/')}/rest/api/3/issue"
    auth = (email, token)
    headers = {"Accept": "application/json", "Content-Type": "application/json"}

    adf_desc = {
        "type": "doc",
        "version": 1,
        "content": [
            _adf_paragraph(description or ""),
            _adf_paragraph("‚Äî —Å–æ–∑–¥–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: AI Secretary (Deepgram ‚Üí Jira)"),
        ],
    }

    payload = {
        "fields": {
            "project": {"key": project_key},
            "summary": (summary or "Task")[:254],
            "issuetype": {"name": "Task"},
            "description": adf_desc,
        }
    }
    r = requests.post(url, auth=auth, headers=headers, json=payload, timeout=30)
    if r.status_code not in (200, 201):
        raise RuntimeError(f"Jira error {r.status_code}: {r.text[:600]}")
    return r.json().get("key", "?")

# ================= UI: –∑–∞–≥—Ä—É–∑–∫–∞ =================
up = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ (mp3, wav, ogg, m4a, mp4, mkv)",
    type=["mp3", "wav", "ogg", "m4a", "mp4", "mkv"]
)

# ================= –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ =================
if up:
    st.audio(up.getvalue())
    if st.button("üéôÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∏ –≤—ã–¥–µ–ª–∏—Ç—å –∑–∞–¥–∞—á–∏"):
        # Deepgram –∫–ª—é—á –∏–∑ Secrets/ENV
        DG_KEY = st.secrets.get("DEEPGRAM_API_KEY") or os.getenv("DEEPGRAM_API_KEY")
        if not DG_KEY:
            st.error("‚ùå –ù–µ—Ç –∫–ª—é—á–∞ Deepgram. –î–æ–±–∞–≤—å—Ç–µ DEEPGRAM_API_KEY –≤ Secrets (Streamlit Cloud) –∏–ª–∏ env.")
            st.stop()
        dg = Deepgram(DG_KEY)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º mimetype
        name_lower = up.name.lower()
        if name_lower.endswith(".wav"):
            mimetype = "audio/wav"
        elif name_lower.endswith(".mp3"):
            mimetype = "audio/mpeg"
        elif name_lower.endswith(".m4a"):
            mimetype = "audio/m4a"
        elif name_lower.endswith(".ogg"):
            mimetype = "audio/ogg"
        elif name_lower.endswith(".mp4"):
            mimetype = "video/mp4"
        elif name_lower.endswith(".mkv"):
            mimetype = "video/x-matroska"
        else:
            # –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å
            mimetype = up.type or "application/octet-stream"

        with st.spinner("–†–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤ Deepgram‚Ä¶"):
            source = {"buffer": io.BytesIO(up.getvalue()), "mimetype": mimetype}
            try:
                res = dg.transcription.sync_prerecorded(
                    source,
                    {
                        "punctuate": True,
                        "smart_format": True,
                        # –º–æ–∂–Ω–æ "auto", –Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π –ª—É—á—à–µ "ru"
                        "language": "ru",
                    },
                )
            except Exception as e:
                st.error(f"Deepgram error: {e}")
                st.stop()

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–æ—Å—Ç–∞–Ω–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
        transcript = ""
        try:
            transcript = (
                res.get("results", {})
                   .get("channels", [{}])[0]
                   .get("alternatives", [{}])[0]
                   .get("transcript", "")
                   .strip()
            )
        except Exception:
            transcript = ""

        if not transcript:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ Deepgram: {json.dumps(res)[:600]}")
            st.stop()

        st.session_state["transcript"] = transcript

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ NLI
        st.info("üß† –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–¥–∞—á–∏ (NLI-—Ñ–∏–ª—å—Ç—Ä, –±–µ—Å–ø–ª–∞—Ç–Ω–æ)‚Ä¶")
        st.session_state["tasks"] = extract_tasks_with_nli(transcript)

# ================= –í—ã–≤–æ–¥ =================
if "transcript" in st.session_state:
    st.subheader("üìÑ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
    st.text_area("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç", st.session_state["transcript"], height=220)

if "tasks" in st.session_state:
    st.subheader("üìå –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏")
    tasks = st.session_state["tasks"]
    if tasks:
        for i, t in enumerate(tasks, 1):
            st.markdown(f"- **{i}.** {t}")
    else:
        st.info("–ù–µ –Ω–∞—à—ë–ª —è–≤–Ω—ã—Ö –ø–æ—Ä—É—á–µ–Ω–∏–π. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Ñ—Ä–∞–∑—ã —Å –≥–ª–∞–≥–æ–ª–∞–º–∏ –¥–µ–π—Å—Ç–≤–∏—è.")

    st.divider()
    st.subheader("üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–¥–∞—á –≤ Jira")
    with st.form("jira_form"):
        jira_base   = st.text_input("Jira Base URL", placeholder="https://your-org.atlassian.net")
        jira_email  = st.text_input("Jira Email", placeholder="you@example.com")
        jira_token  = st.text_input("Jira API Token", type="password")
        jira_proj   = st.text_input("Project Key", placeholder="TEST")
        send = st.form_submit_button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ Jira ‚úÖ")

    if send:
        if not (jira_base and jira_email and jira_token and jira_proj):
            st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è Jira.")
        else:
            results = []
            bar = st.progress(0, text="–°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –≤ Jira‚Ä¶")
            total = max(1, len(tasks))
            for i, task in enumerate(tasks, 1):
                try:
                    key = create_jira_task(jira_base, jira_email, jira_token, jira_proj,
                                           summary=task, description=task)
                    results.append(("ok", key, task))
                except Exception as e:
                    results.append(("err", str(e), task))
                bar.progress(i/total)
            st.subheader("üßæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á")
            ok = [r for r in results if r[0] == "ok"]
            err = [r for r in results if r[0] == "err"]
            if ok:
                st.success("–°–æ–∑–¥–∞–Ω—ã: " + ", ".join([k for _, k, _ in ok]))
            for _, msg, t in err:
                st.error(f"–ù–µ —Å–æ–∑–¥–∞–Ω–∞: ‚Äû{t}‚Äú ‚Üí {msg}")
